{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2eae79-de3c-4b5b-ab38-9dcb6c914a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you're on Colab or Kaggle\n",
    "# !git clone https://github.com/othrif/deeplearn-2022au-speech-language.git\n",
    "# %cd deeplearn-2022au-speech-language\n",
    "# from install import *\n",
    "# install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49f2390-6882-408d-8f36-f729dcb3706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected! This notebook can be *very* slow without a GPU\n",
      "Using transformers v4.11.3\n",
      "Using datasets v1.16.1\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "setup_notebooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0e1c1-e819-42c3-a23c-27f81efff33b",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5564e8-25c6-4f92-a0cb-058d4e53d5e5",
   "metadata": {},
   "source": [
    "In this exercise, we will be using a variant of BERT called [DistilBERT](https://arxiv.org/abs/1910.01108). The main advantage of this model is that it achieves comparable performance to BERT, while being significantly smaller and more efficient. This enables us to train a classifier in a few minutes, and if you want to train a larger BERT model you can simply change the checkpoint of the pretrained model. A _checkpoint_ corresponds to the set of weights that are loaded into a given transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420a65d-75af-4453-ba59-d69b64384699",
   "metadata": {},
   "source": [
    "<img alt=\"Hugging Face Pipeline\" caption=\"A typical pipeline for training transformer models\" src=\"figs/hf-libraries.png\" id=\"hf-libraries\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76f90e-6905-4f30-af9a-fd1eda1f82a0",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "We will build an emotion detector by leveraging data from [CARER](http://dx.doi.org/10.18653/v1/D18-1404) that explored how emotions are represented in English Twitter messages. This dataset contains six basic emotions: anger, disgust, fear, joy, sadness, and surprise. Given a tweet, our task will be to train a model that can classify it into one of these emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a5b9e5-7aa6-4bc1-962c-82a1c44be928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2107dbfc333c47e09516e954f7ec787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaea00-a625-42e8-8cea-5c3185671917",
   "metadata": {},
   "source": [
    "## The Tokenizer\n",
    "\n",
    "Transformer models like DistilBERT cannot receive raw strings as input; instead, they assume the text has been _tokenized_ and _encoded_ as numerical vectors. Tokenization is the step of breaking down a string into the atomic units used in the model. For the purpose of this exercise, we will use [WordPiece](https://doi.org/10.1109/ICASSP.2012.6289079) which is used by the BERT and DistilBERT tokenizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dfdc21-7d16-4169-a4f4-209af784f55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 06:21:52.845252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4866a6-040c-4dff-a860-1861252dea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3419389e-e7cd-4439-a61a-1adcf6fda382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a3b42d-0d32-4389-8ab9-50beae02fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac14ceed-fe7f-43f6-886d-166ca3a6dfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b35523d-facc-4892-9783-addcbfe742b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1630419a838d4825ba62885d02e9f8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adfc9b105434ea2b05a530476a968f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8369f6caece40a2ad1178dee11bf3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the whole dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efce8c4-9a95-4234-a5d5-12fa583e6977",
   "metadata": {},
   "source": [
    "## The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615880c9-dbd8-4c34-b364-9d135653f159",
   "metadata": {},
   "source": [
    "### Transformer as feature extractor\n",
    "\n",
    "In order to use a transformer as a feature extractor, we freeze the body's weights during training and use the hidden states as features for the classifier. The advantage of this approach is that we can quickly train a small or shallow model. Such a model could be a neural classification layer. This method is especially convenient if GPUs are unavailable, since the hidden states only need to be precomputed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70207140-dd7c-47ca-a089-612f2c9de494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63a2537d-714a-4baf-ba00-25d2d6b7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3397f094-59a0-427c-b1f6-1c541d16ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127c65e-d6f3-4cc0-b38f-754f40e1ceb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81de9ba0c7e44c1fb158788fd43a8fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dff22-90bf-4cce-afaa-b1b8f11136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8697f75-4c6f-43b3-9a4c-359e5a659fcb",
   "metadata": {},
   "source": [
    "#### Add a basic layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f425b0-9fdc-4818-a08a-5717728ed20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d83f1-114e-4c01-b7b2-6aec4fd3bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edb7dd-ba82-4dd5-97fa-8e1240a9f05d",
   "metadata": {},
   "source": [
    "#### Compare with true random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec3482-b704-437d-957c-78c9492ff941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0d51a-5065-4795-b76c-127b418120e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
